{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model for predicting trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "import sklearn\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "import keras.utils\n",
    "from keras.layers import LSTM\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import tensorflow as tf\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import seed\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    \n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "        \n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate rsi\n",
    "def rsi(ohlc, period: int = 14) -> pd.Series:\n",
    "    \"\"\"See source https://github.com/peerchemist/finta\n",
    "    and fix https://www.tradingview.com/wiki/Talk:Relative_Strength_Index_(RSI)\n",
    "    Relative Strength Index (RSI) is a momentum oscillator that measures the speed and change of price movements.\n",
    "    RSI oscillates between zero and 100. Traditionally, and according to Wilder, RSI is considered overbought when above 70 and oversold when below 30.\n",
    "    Signals can also be generated by looking for divergences, failure swings and centerline crossovers.\n",
    "    RSI can also be used to identify the general trend.\"\"\"\n",
    "\n",
    "    delta = ohlc[\"Close\"].diff()\n",
    "\n",
    "    up, down = delta.copy(), delta.copy()\n",
    "    up[up < 0] = 0\n",
    "    down[down > 0] = 0\n",
    "\n",
    "    _gain = up.ewm(com=(period - 1), min_periods=period).mean()\n",
    "    _loss = down.abs().ewm(com=(period - 1), min_periods=period).mean()\n",
    "\n",
    "    RS = _gain / _loss\n",
    "    return 100 - (100 / (1 + RS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lag granularity - days or hours\n",
    "lag_granularity = \"days\"\n",
    "#lag value\n",
    "lag = 7\n",
    "# type of analyser - TextBlob or vader\n",
    "analyser = \"vader\"\n",
    "# analyser = \"TextBlob\"\n",
    "#dataset grouped type - day or hour\n",
    "dataset_grouped_by = \"day\"\n",
    "cleaned = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset\n",
    "folder = \"./../datasets/tweets_prices_volumes_sentiment/\"+analyser+\"/\"+dataset_grouped_by+\"_datasets/\"\n",
    "\n",
    "if(cleaned):\n",
    "    folder = folder+\"cleaned\"\n",
    "filename = folder+\"/final_data_lag_\"+lag_granularity+\"_\"+str(lag)+\".csv\" if (lag > 0) else folder+\"/final_data_no_lag.csv\"\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by datetime\n",
    "df = df.groupby('DateTime').agg(lambda x: x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate change\n",
    "df[\"Change\"] = (df[\"Close\"] - df[\"Close\"].shift(1)).astype(float)\n",
    "# #drop empty\n",
    "df = df.dropna(subset=['Change'])\n",
    "#max positive change \n",
    "max_change = df[\"Change\"].max()\n",
    "#max negative change \n",
    "min_change = df[\"Change\"].min()\n",
    "\n",
    "#prepare bins\n",
    "rnge = max_change - min_change\n",
    "bin_size = (max_change - min_change) / 10\n",
    "half_range = rnge/2\n",
    "bins = np.arange(-1*half_range, half_range, bin_size)\n",
    "bins[5] = 0\n",
    "bins[0] = float(\"-inf\")\n",
    "bins = np.append(bins, float(\"inf\"))\n",
    "labels = [0, 1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "# #set bins\n",
    "df['Change'] = pd.cut(x=df['Change'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "add_RSI = False\n",
    "add_longMAvg = False\n",
    "add_shortMAvg = False\n",
    "\n",
    "if(add_RSI):\n",
    "    #calcualte RSI\n",
    "    RSI = 14\n",
    "    df['RSI'] = rsi(df, RSI)\n",
    "    df = df.iloc[RSI:]\n",
    "\n",
    "#calculate moving averages\n",
    "if(add_shortMAvg):\n",
    "    short_window = 9\n",
    "    df['short_mavg'] = df.rolling(window=short_window)[\"Close\"].mean()\n",
    "    \n",
    "if(add_longMAvg):\n",
    "    long_window = 21\n",
    "    df[\"long_mavg\"] = df.rolling(window=long_window)[\"Close\"].mean()\n",
    "    \n",
    "if(add_longMAvg):\n",
    "    df = df.iloc[long_window:]\n",
    "elif(add_RSI):\n",
    "    df = df.iloc[RSI:]\n",
    "elif(add_shortMAvg):\n",
    "    df = df.iloc[short_window:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Change</th>\n",
       "      <th>Close</th>\n",
       "      <th>pos_pol</th>\n",
       "      <th>neg_pol</th>\n",
       "      <th>Tweet_vol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-06-13 00:00:00+00:00</th>\n",
       "      <td>5</td>\n",
       "      <td>9517.76</td>\n",
       "      <td>0.180987</td>\n",
       "      <td>0.059525</td>\n",
       "      <td>62462.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-14 00:00:00+00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>9920.00</td>\n",
       "      <td>0.173148</td>\n",
       "      <td>0.070565</td>\n",
       "      <td>71947.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-15 00:00:00+00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>10564.70</td>\n",
       "      <td>0.175750</td>\n",
       "      <td>0.065463</td>\n",
       "      <td>56042.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-16 00:00:00+00:00</th>\n",
       "      <td>5</td>\n",
       "      <td>10888.39</td>\n",
       "      <td>0.184176</td>\n",
       "      <td>0.062727</td>\n",
       "      <td>69335.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-17 00:00:00+00:00</th>\n",
       "      <td>5</td>\n",
       "      <td>10930.01</td>\n",
       "      <td>0.183068</td>\n",
       "      <td>0.060818</td>\n",
       "      <td>195001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-18 00:00:00+00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>11555.00</td>\n",
       "      <td>0.178610</td>\n",
       "      <td>0.065571</td>\n",
       "      <td>78073.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-19 00:00:00+00:00</th>\n",
       "      <td>7</td>\n",
       "      <td>12461.63</td>\n",
       "      <td>0.184258</td>\n",
       "      <td>0.061996</td>\n",
       "      <td>68038.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-20 00:00:00+00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>10715.14</td>\n",
       "      <td>0.180243</td>\n",
       "      <td>0.061412</td>\n",
       "      <td>123865.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-21 00:00:00+00:00</th>\n",
       "      <td>9</td>\n",
       "      <td>12278.42</td>\n",
       "      <td>0.180027</td>\n",
       "      <td>0.064185</td>\n",
       "      <td>130824.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-22 00:00:00+00:00</th>\n",
       "      <td>3</td>\n",
       "      <td>11946.02</td>\n",
       "      <td>0.180491</td>\n",
       "      <td>0.068335</td>\n",
       "      <td>139151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-23 00:00:00+00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>11262.60</td>\n",
       "      <td>0.182320</td>\n",
       "      <td>0.061431</td>\n",
       "      <td>72620.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-24 00:00:00+00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>10560.24</td>\n",
       "      <td>0.184518</td>\n",
       "      <td>0.060844</td>\n",
       "      <td>119732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-25 00:00:00+00:00</th>\n",
       "      <td>5</td>\n",
       "      <td>10675.94</td>\n",
       "      <td>0.176406</td>\n",
       "      <td>0.064476</td>\n",
       "      <td>91885.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-26 00:00:00+00:00</th>\n",
       "      <td>7</td>\n",
       "      <td>11407.72</td>\n",
       "      <td>0.173243</td>\n",
       "      <td>0.075330</td>\n",
       "      <td>141641.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-27 00:00:00+00:00</th>\n",
       "      <td>5</td>\n",
       "      <td>11561.88</td>\n",
       "      <td>0.170833</td>\n",
       "      <td>0.072952</td>\n",
       "      <td>103556.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-28 00:00:00+00:00</th>\n",
       "      <td>3</td>\n",
       "      <td>11084.56</td>\n",
       "      <td>0.175137</td>\n",
       "      <td>0.065832</td>\n",
       "      <td>81949.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-29 00:00:00+00:00</th>\n",
       "      <td>5</td>\n",
       "      <td>11168.97</td>\n",
       "      <td>0.177400</td>\n",
       "      <td>0.062968</td>\n",
       "      <td>66809.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30 00:00:00+00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>11545.45</td>\n",
       "      <td>0.180215</td>\n",
       "      <td>0.064507</td>\n",
       "      <td>63816.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01 00:00:00+00:00</th>\n",
       "      <td>7</td>\n",
       "      <td>12347.17</td>\n",
       "      <td>0.179705</td>\n",
       "      <td>0.060119</td>\n",
       "      <td>79008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-02 00:00:00+00:00</th>\n",
       "      <td>5</td>\n",
       "      <td>12562.12</td>\n",
       "      <td>0.177948</td>\n",
       "      <td>0.067190</td>\n",
       "      <td>84127.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Change     Close   pos_pol   neg_pol  Tweet_vol\n",
       "DateTime                                                                 \n",
       "2019-06-13 00:00:00+00:00      5   9517.76  0.180987  0.059525    62462.0\n",
       "2019-06-14 00:00:00+00:00      6   9920.00  0.173148  0.070565    71947.0\n",
       "2019-06-15 00:00:00+00:00      6  10564.70  0.175750  0.065463    56042.0\n",
       "2019-06-16 00:00:00+00:00      5  10888.39  0.184176  0.062727    69335.0\n",
       "2019-06-17 00:00:00+00:00      5  10930.01  0.183068  0.060818   195001.0\n",
       "2019-06-18 00:00:00+00:00      6  11555.00  0.178610  0.065571    78073.0\n",
       "2019-06-19 00:00:00+00:00      7  12461.63  0.184258  0.061996    68038.0\n",
       "2019-06-20 00:00:00+00:00      0  10715.14  0.180243  0.061412   123865.0\n",
       "2019-06-21 00:00:00+00:00      9  12278.42  0.180027  0.064185   130824.0\n",
       "2019-06-22 00:00:00+00:00      3  11946.02  0.180491  0.068335   139151.0\n",
       "2019-06-23 00:00:00+00:00      2  11262.60  0.182320  0.061431    72620.0\n",
       "2019-06-24 00:00:00+00:00      2  10560.24  0.184518  0.060844   119732.0\n",
       "2019-06-25 00:00:00+00:00      5  10675.94  0.176406  0.064476    91885.0\n",
       "2019-06-26 00:00:00+00:00      7  11407.72  0.173243  0.075330   141641.0\n",
       "2019-06-27 00:00:00+00:00      5  11561.88  0.170833  0.072952   103556.0\n",
       "2019-06-28 00:00:00+00:00      3  11084.56  0.175137  0.065832    81949.0\n",
       "2019-06-29 00:00:00+00:00      5  11168.97  0.177400  0.062968    66809.0\n",
       "2019-06-30 00:00:00+00:00      6  11545.45  0.180215  0.064507    63816.0\n",
       "2019-07-01 00:00:00+00:00      7  12347.17  0.179705  0.060119    79008.0\n",
       "2019-07-02 00:00:00+00:00      5  12562.12  0.177948  0.067190    84127.0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[290:310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only wanted columns\n",
    "features = ['Change', 'subjectivity', 'polarity','Tweet_vol','Volume_(BTC)'] if analyser == \"Textblob\" else ['Change', 'Close', 'pos_pol', 'neg_pol', 'Tweet_vol']\n",
    "\n",
    "if(add_RSI):\n",
    "    features.append(\"RSI\")\n",
    "    \n",
    "if(add_longMAvg):\n",
    "    features.append(\"long_mavg\")\n",
    "    \n",
    "if(add_shortMAvg):\n",
    "    features.append(\"short_mavg\")\n",
    "\n",
    "df = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot correlation matrix\n",
    "sn.heatmap(df.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating copy so that data is not loaded once again\n",
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Change', 'Close', 'pos_pol', 'neg_pol', 'Tweet_vol']\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of previous records to consider for every example\n",
    "n_lag = 14\n",
    "#number of features\n",
    "n_features = len(features)\n",
    "#calculate total_features\n",
    "total_features = n_lag*n_features\n",
    "\n",
    "if(total_features == 0):\n",
    "    total_features = n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide df into train and test\n",
    "train_ratio = 0.85\n",
    "data_len = len(df_copy)\n",
    "train_size = int(data_len*train_ratio)\n",
    "\n",
    "train = df_copy.iloc[:train_size]\n",
    "test = df_copy.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare labels\n",
    "train_y = train[\"Change\"][n_lag:].values\n",
    "test_y = test[\"Change\"][n_lag:].values\n",
    "\n",
    "train_y = train_y.reshape(len(train_y), 1)\n",
    "test_y = test_y.reshape(len(test_y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise features\n",
    "xscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train = xscaler.fit_transform(train)\n",
    "test = xscaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add lagged data to records\n",
    "train_reframed = series_to_supervised(train, n_lag, 1)\n",
    "train_reframed =  train_reframed.reset_index()\n",
    "train_reframed = train_reframed.drop(['index'], axis=1)\n",
    "\n",
    "test_reframed = series_to_supervised(test, n_lag, 1)\n",
    "test_reframed =  test_reframed.reset_index()\n",
    "test_reframed = test_reframed.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols_to_rem_label(n_lag, n_features, label_col):\n",
    "    to_rem = []\n",
    "    for i in range(1, n_features+1):\n",
    "        for j in range(0, n_lag+1):\n",
    "            if(j!=0 or i != label_col):\n",
    "                if(j == 0):\n",
    "                    to_rem.append(\"var\"+str(i)+\"(t)\")\n",
    "                else:\n",
    "                    to_rem.append(\"var\"+str(i)+\"(t-\"+str(j)+\")\")\n",
    "    return to_rem\n",
    "    \n",
    "def get_cols_to_rem_feat(n_lag, n_features, label_col):\n",
    "    #uncomment the below line to only remove the last price\n",
    "    to_rem = [\"var1(t)\", \"var2(t)\"]\n",
    "    return to_rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reframed = train_reframed.drop(get_cols_to_rem_feat(n_lag, n_features, 1), axis=1)\n",
    "test_reframed = test_reframed.drop(get_cols_to_rem_feat(n_lag, n_features, 1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tprepare data\n",
    "train = train_reframed.values\n",
    "test = test_reframed.values\n",
    "train_labels = train_y\n",
    "test_labels = test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the last set of values(data of time to be predicted)\n",
    "train = train[:, :total_features]\n",
    "test = test[:, :total_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only prices array\n",
    "train_X, train_y = train[:, :total_features], train_y[:, -1]\n",
    "test_X, test_y = test[:, :total_features], test_y[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "if(n_lag > 0):\n",
    "    train_X = train_X.reshape((train_X.shape[0], n_lag, n_features))\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_lag, n_features))\n",
    "else:\n",
    "    train_X = train_X.reshape((train_X.shape[0], 1, n_features-1))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, n_features-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 5, 5, 5, ..., 4, 5, 4, 4, 5]\n",
       "Length: 371\n",
       "Categories (10, int64): [0 < 1 < 2 < 3 ... 6 < 7 < 8 < 9]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set labels for training data to categorical\n",
    "train_y = keras.utils.to_categorical(train_y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 14, 32)            4864      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 14, 32)            8320      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 21,834\n",
      "Trainable params: 21,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "60/60 - 1s - loss: 1.5773 - accuracy: 0.4223 - val_loss: 1.7205 - val_accuracy: 0.3600\n",
      "Epoch 2/200\n",
      "60/60 - 1s - loss: 1.4921 - accuracy: 0.4527 - val_loss: 1.6607 - val_accuracy: 0.3600\n",
      "Epoch 3/200\n",
      "60/60 - 1s - loss: 1.2864 - accuracy: 0.4764 - val_loss: 1.6484 - val_accuracy: 0.3600\n",
      "Epoch 4/200\n",
      "60/60 - 1s - loss: 1.2029 - accuracy: 0.4595 - val_loss: 1.6435 - val_accuracy: 0.3600\n",
      "Epoch 5/200\n",
      "60/60 - 1s - loss: 1.1552 - accuracy: 0.4899 - val_loss: 1.6522 - val_accuracy: 0.3600\n",
      "Epoch 6/200\n",
      "60/60 - 1s - loss: 1.1319 - accuracy: 0.5034 - val_loss: 1.6768 - val_accuracy: 0.3600\n",
      "Epoch 7/200\n",
      "60/60 - 1s - loss: 1.1462 - accuracy: 0.4966 - val_loss: 1.6991 - val_accuracy: 0.3600\n",
      "Epoch 8/200\n",
      "60/60 - 1s - loss: 1.1446 - accuracy: 0.5034 - val_loss: 1.7228 - val_accuracy: 0.3600\n",
      "Epoch 9/200\n",
      "60/60 - 1s - loss: 1.1252 - accuracy: 0.4966 - val_loss: 1.7464 - val_accuracy: 0.3600\n",
      "Epoch 10/200\n",
      "60/60 - 1s - loss: 1.1298 - accuracy: 0.4932 - val_loss: 1.7574 - val_accuracy: 0.3600\n",
      "Epoch 11/200\n",
      "60/60 - 1s - loss: 1.0914 - accuracy: 0.5439 - val_loss: 1.7626 - val_accuracy: 0.3600\n",
      "Epoch 12/200\n",
      "60/60 - 1s - loss: 1.1215 - accuracy: 0.4797 - val_loss: 1.7855 - val_accuracy: 0.3600\n",
      "Epoch 13/200\n",
      "60/60 - 1s - loss: 1.1074 - accuracy: 0.4764 - val_loss: 1.7462 - val_accuracy: 0.3600\n",
      "Epoch 14/200\n",
      "60/60 - 1s - loss: 1.1287 - accuracy: 0.4899 - val_loss: 1.7402 - val_accuracy: 0.3600\n",
      "Epoch 15/200\n",
      "60/60 - 1s - loss: 1.1123 - accuracy: 0.4730 - val_loss: 1.7367 - val_accuracy: 0.3600\n",
      "Epoch 16/200\n",
      "60/60 - 1s - loss: 1.1135 - accuracy: 0.4797 - val_loss: 1.7638 - val_accuracy: 0.3600\n",
      "Epoch 17/200\n",
      "60/60 - 1s - loss: 1.0993 - accuracy: 0.4561 - val_loss: 1.8931 - val_accuracy: 0.3600\n",
      "Epoch 18/200\n",
      "60/60 - 1s - loss: 1.1531 - accuracy: 0.4865 - val_loss: 1.5944 - val_accuracy: 0.3600\n",
      "Epoch 19/200\n",
      "60/60 - 1s - loss: 1.0917 - accuracy: 0.5034 - val_loss: 1.6178 - val_accuracy: 0.3600\n",
      "Epoch 20/200\n",
      "60/60 - 1s - loss: 1.1106 - accuracy: 0.4730 - val_loss: 1.6446 - val_accuracy: 0.3600\n",
      "Epoch 21/200\n",
      "60/60 - 1s - loss: 1.1019 - accuracy: 0.4899 - val_loss: 1.6611 - val_accuracy: 0.3600\n",
      "Epoch 22/200\n",
      "60/60 - 1s - loss: 1.1055 - accuracy: 0.4797 - val_loss: 1.7043 - val_accuracy: 0.3600\n",
      "Epoch 23/200\n",
      "60/60 - 1s - loss: 1.0953 - accuracy: 0.4899 - val_loss: 1.7949 - val_accuracy: 0.3600\n",
      "Epoch 24/200\n",
      "60/60 - 1s - loss: 1.0838 - accuracy: 0.4899 - val_loss: 1.7868 - val_accuracy: 0.3600\n",
      "Epoch 25/200\n",
      "60/60 - 1s - loss: 1.0939 - accuracy: 0.4561 - val_loss: 1.7882 - val_accuracy: 0.3600\n",
      "Epoch 26/200\n",
      "60/60 - 1s - loss: 1.0787 - accuracy: 0.4730 - val_loss: 1.7594 - val_accuracy: 0.3600\n",
      "Epoch 27/200\n",
      "60/60 - 1s - loss: 1.0905 - accuracy: 0.5169 - val_loss: 1.7472 - val_accuracy: 0.3600\n",
      "Epoch 28/200\n",
      "60/60 - 1s - loss: 1.1047 - accuracy: 0.4527 - val_loss: 1.7310 - val_accuracy: 0.3600\n",
      "Epoch 29/200\n",
      "60/60 - 1s - loss: 1.0993 - accuracy: 0.4797 - val_loss: 1.8059 - val_accuracy: 0.3600\n",
      "Epoch 30/200\n",
      "60/60 - 1s - loss: 1.0992 - accuracy: 0.4797 - val_loss: 1.8082 - val_accuracy: 0.3600\n",
      "Epoch 31/200\n",
      "60/60 - 1s - loss: 1.0851 - accuracy: 0.4459 - val_loss: 1.8054 - val_accuracy: 0.3600\n",
      "Epoch 32/200\n",
      "60/60 - 1s - loss: 1.0915 - accuracy: 0.4797 - val_loss: 1.8074 - val_accuracy: 0.3600\n",
      "Epoch 33/200\n",
      "60/60 - 1s - loss: 1.0788 - accuracy: 0.4730 - val_loss: 1.8701 - val_accuracy: 0.3600\n",
      "Epoch 34/200\n",
      "60/60 - 1s - loss: 1.0812 - accuracy: 0.4764 - val_loss: 1.8040 - val_accuracy: 0.3600\n",
      "Epoch 35/200\n",
      "60/60 - 1s - loss: 1.0795 - accuracy: 0.4797 - val_loss: 1.6649 - val_accuracy: 0.3600\n",
      "Epoch 36/200\n",
      "60/60 - 1s - loss: 1.0786 - accuracy: 0.4932 - val_loss: 1.8661 - val_accuracy: 0.3600\n",
      "Epoch 37/200\n",
      "60/60 - 1s - loss: 1.0888 - accuracy: 0.5000 - val_loss: 2.0062 - val_accuracy: 0.3600\n",
      "Epoch 38/200\n",
      "60/60 - 1s - loss: 1.0805 - accuracy: 0.4696 - val_loss: 1.8579 - val_accuracy: 0.3600\n",
      "Epoch 39/200\n",
      "60/60 - 1s - loss: 1.1030 - accuracy: 0.4831 - val_loss: 1.8081 - val_accuracy: 0.3600\n",
      "Epoch 40/200\n",
      "60/60 - 1s - loss: 1.0758 - accuracy: 0.5034 - val_loss: 1.8060 - val_accuracy: 0.3600\n",
      "Epoch 41/200\n",
      "60/60 - 1s - loss: 1.0552 - accuracy: 0.5000 - val_loss: 1.7526 - val_accuracy: 0.3600\n",
      "Epoch 42/200\n",
      "60/60 - 1s - loss: 1.0748 - accuracy: 0.4730 - val_loss: 1.7748 - val_accuracy: 0.3600\n",
      "Epoch 43/200\n",
      "60/60 - 1s - loss: 1.0646 - accuracy: 0.5000 - val_loss: 1.9235 - val_accuracy: 0.3600\n",
      "Epoch 44/200\n",
      "60/60 - 1s - loss: 1.0843 - accuracy: 0.5372 - val_loss: 1.7144 - val_accuracy: 0.3600\n",
      "Epoch 45/200\n",
      "60/60 - 1s - loss: 1.0796 - accuracy: 0.5101 - val_loss: 1.6682 - val_accuracy: 0.3600\n",
      "Epoch 46/200\n",
      "60/60 - 1s - loss: 1.0658 - accuracy: 0.5169 - val_loss: 1.8669 - val_accuracy: 0.3600\n",
      "Epoch 47/200\n",
      "60/60 - 1s - loss: 1.0520 - accuracy: 0.5000 - val_loss: 2.3681 - val_accuracy: 0.3600\n",
      "Epoch 48/200\n",
      "60/60 - 1s - loss: 1.0789 - accuracy: 0.5270 - val_loss: 1.7315 - val_accuracy: 0.3600\n",
      "Epoch 49/200\n",
      "60/60 - 1s - loss: 1.0698 - accuracy: 0.4932 - val_loss: 1.6902 - val_accuracy: 0.3600\n",
      "Epoch 50/200\n",
      "60/60 - 1s - loss: 1.0737 - accuracy: 0.4966 - val_loss: 1.7023 - val_accuracy: 0.3600\n",
      "Epoch 51/200\n",
      "60/60 - 1s - loss: 1.0562 - accuracy: 0.4797 - val_loss: 1.7223 - val_accuracy: 0.3600\n",
      "Epoch 52/200\n",
      "60/60 - 1s - loss: 1.0266 - accuracy: 0.4797 - val_loss: 1.8163 - val_accuracy: 0.3600\n",
      "Epoch 53/200\n",
      "60/60 - 1s - loss: 1.0634 - accuracy: 0.5236 - val_loss: 1.8273 - val_accuracy: 0.3600\n",
      "Epoch 54/200\n",
      "60/60 - 1s - loss: 1.0453 - accuracy: 0.4865 - val_loss: 2.0122 - val_accuracy: 0.3600\n",
      "Epoch 55/200\n",
      "60/60 - 1s - loss: 1.0653 - accuracy: 0.4831 - val_loss: 2.1200 - val_accuracy: 0.3600\n",
      "Epoch 56/200\n",
      "60/60 - 1s - loss: 1.0398 - accuracy: 0.4764 - val_loss: 2.3657 - val_accuracy: 0.3600\n",
      "Epoch 57/200\n",
      "60/60 - 1s - loss: 1.1285 - accuracy: 0.4899 - val_loss: 1.6402 - val_accuracy: 0.3600\n",
      "Epoch 58/200\n",
      "60/60 - 1s - loss: 1.0782 - accuracy: 0.4730 - val_loss: 1.6561 - val_accuracy: 0.3600\n",
      "Epoch 59/200\n",
      "60/60 - 1s - loss: 1.0793 - accuracy: 0.4865 - val_loss: 1.7276 - val_accuracy: 0.3600\n",
      "Epoch 60/200\n",
      "60/60 - 1s - loss: 1.0609 - accuracy: 0.5270 - val_loss: 1.7668 - val_accuracy: 0.3600\n",
      "Epoch 61/200\n",
      "60/60 - 1s - loss: 1.0475 - accuracy: 0.5236 - val_loss: 1.7163 - val_accuracy: 0.3600\n",
      "Epoch 62/200\n",
      "60/60 - 1s - loss: 1.0530 - accuracy: 0.5101 - val_loss: 1.7894 - val_accuracy: 0.3600\n",
      "Epoch 63/200\n",
      "60/60 - 1s - loss: 1.0539 - accuracy: 0.4730 - val_loss: 1.7651 - val_accuracy: 0.3600\n",
      "Epoch 64/200\n",
      "60/60 - 1s - loss: 1.0557 - accuracy: 0.5000 - val_loss: 1.6644 - val_accuracy: 0.3600\n",
      "Epoch 65/200\n",
      "60/60 - 1s - loss: 1.0707 - accuracy: 0.5169 - val_loss: 1.7123 - val_accuracy: 0.3600\n",
      "Epoch 66/200\n",
      "60/60 - 1s - loss: 1.0404 - accuracy: 0.4932 - val_loss: 1.7417 - val_accuracy: 0.3600\n",
      "Epoch 67/200\n",
      "60/60 - 1s - loss: 1.0481 - accuracy: 0.5169 - val_loss: 1.8315 - val_accuracy: 0.3600\n",
      "Epoch 68/200\n",
      "60/60 - 1s - loss: 1.0418 - accuracy: 0.5000 - val_loss: 2.0218 - val_accuracy: 0.3600\n",
      "Epoch 69/200\n",
      "60/60 - 1s - loss: 1.0337 - accuracy: 0.4966 - val_loss: 2.1860 - val_accuracy: 0.3467\n",
      "Epoch 70/200\n",
      "60/60 - 1s - loss: 1.0034 - accuracy: 0.5372 - val_loss: 2.3909 - val_accuracy: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/200\n",
      "60/60 - 1s - loss: 1.0281 - accuracy: 0.4764 - val_loss: 1.9526 - val_accuracy: 0.3333\n",
      "Epoch 72/200\n",
      "60/60 - 1s - loss: 1.0204 - accuracy: 0.4865 - val_loss: 2.0462 - val_accuracy: 0.3467\n",
      "Epoch 73/200\n",
      "60/60 - 1s - loss: 1.0372 - accuracy: 0.4831 - val_loss: 2.0814 - val_accuracy: 0.2933\n",
      "Epoch 74/200\n",
      "60/60 - 1s - loss: 1.1205 - accuracy: 0.4865 - val_loss: 1.7452 - val_accuracy: 0.3600\n",
      "Epoch 75/200\n",
      "60/60 - 1s - loss: 1.0450 - accuracy: 0.5135 - val_loss: 1.6675 - val_accuracy: 0.3600\n",
      "Epoch 76/200\n",
      "60/60 - 1s - loss: 1.0597 - accuracy: 0.5000 - val_loss: 1.7140 - val_accuracy: 0.3600\n",
      "Epoch 77/200\n",
      "60/60 - 1s - loss: 1.0353 - accuracy: 0.5101 - val_loss: 1.8150 - val_accuracy: 0.3600\n",
      "Epoch 78/200\n",
      "60/60 - 1s - loss: 1.0159 - accuracy: 0.4865 - val_loss: 1.9637 - val_accuracy: 0.3600\n",
      "Epoch 79/200\n",
      "60/60 - 1s - loss: 1.0173 - accuracy: 0.4899 - val_loss: 2.1179 - val_accuracy: 0.3600\n",
      "Epoch 80/200\n",
      "60/60 - 1s - loss: 1.0137 - accuracy: 0.5236 - val_loss: 1.9300 - val_accuracy: 0.3600\n",
      "Epoch 81/200\n",
      "60/60 - 1s - loss: 1.0047 - accuracy: 0.4966 - val_loss: 2.1904 - val_accuracy: 0.3600\n",
      "Epoch 82/200\n",
      "60/60 - 1s - loss: 1.0340 - accuracy: 0.4831 - val_loss: 2.1044 - val_accuracy: 0.3333\n",
      "Epoch 83/200\n",
      "60/60 - 1s - loss: 0.9912 - accuracy: 0.4932 - val_loss: 2.3455 - val_accuracy: 0.3600\n",
      "Epoch 84/200\n",
      "60/60 - 1s - loss: 0.9995 - accuracy: 0.5236 - val_loss: 1.9094 - val_accuracy: 0.3467\n",
      "Epoch 85/200\n",
      "60/60 - 1s - loss: 0.9927 - accuracy: 0.5236 - val_loss: 2.3543 - val_accuracy: 0.3600\n",
      "Epoch 86/200\n",
      "60/60 - 1s - loss: 1.0222 - accuracy: 0.5236 - val_loss: 1.6262 - val_accuracy: 0.3600\n",
      "Epoch 87/200\n",
      "60/60 - 1s - loss: 1.0313 - accuracy: 0.5135 - val_loss: 1.8256 - val_accuracy: 0.3600\n",
      "Epoch 88/200\n",
      "60/60 - 1s - loss: 1.0599 - accuracy: 0.5203 - val_loss: 1.8538 - val_accuracy: 0.3600\n",
      "Epoch 89/200\n",
      "60/60 - 1s - loss: 1.0339 - accuracy: 0.5101 - val_loss: 1.9109 - val_accuracy: 0.3467\n",
      "Epoch 90/200\n",
      "60/60 - 1s - loss: 1.0003 - accuracy: 0.4966 - val_loss: 1.8236 - val_accuracy: 0.3333\n",
      "Epoch 91/200\n",
      "60/60 - 1s - loss: 1.0372 - accuracy: 0.4899 - val_loss: 1.8865 - val_accuracy: 0.3333\n",
      "Epoch 92/200\n",
      "60/60 - 1s - loss: 1.1119 - accuracy: 0.4865 - val_loss: 1.6986 - val_accuracy: 0.3600\n",
      "Epoch 93/200\n",
      "60/60 - 1s - loss: 1.1059 - accuracy: 0.5000 - val_loss: 1.5951 - val_accuracy: 0.3600\n",
      "Epoch 94/200\n",
      "60/60 - 1s - loss: 1.0572 - accuracy: 0.5034 - val_loss: 1.6764 - val_accuracy: 0.3600\n",
      "Epoch 95/200\n",
      "60/60 - 1s - loss: 1.0274 - accuracy: 0.5000 - val_loss: 1.7709 - val_accuracy: 0.3600\n",
      "Epoch 96/200\n",
      "60/60 - 1s - loss: 1.0304 - accuracy: 0.4797 - val_loss: 1.8099 - val_accuracy: 0.3333\n",
      "Epoch 97/200\n",
      "60/60 - 1s - loss: 1.0424 - accuracy: 0.5101 - val_loss: 1.8008 - val_accuracy: 0.3333\n",
      "Epoch 98/200\n",
      "60/60 - 1s - loss: 1.0743 - accuracy: 0.5068 - val_loss: 1.6179 - val_accuracy: 0.3733\n",
      "Epoch 99/200\n",
      "60/60 - 1s - loss: 1.0429 - accuracy: 0.4899 - val_loss: 2.0034 - val_accuracy: 0.3333\n",
      "Epoch 100/200\n",
      "60/60 - 1s - loss: 0.9929 - accuracy: 0.5068 - val_loss: 2.4725 - val_accuracy: 0.3467\n",
      "Epoch 101/200\n",
      "60/60 - 1s - loss: 0.9800 - accuracy: 0.5169 - val_loss: 1.9910 - val_accuracy: 0.3333\n",
      "Epoch 102/200\n",
      "60/60 - 1s - loss: 0.9976 - accuracy: 0.5338 - val_loss: 1.8718 - val_accuracy: 0.3333\n",
      "Epoch 103/200\n",
      "60/60 - 1s - loss: 1.0368 - accuracy: 0.5000 - val_loss: 1.7904 - val_accuracy: 0.3333\n",
      "Epoch 104/200\n",
      "60/60 - 1s - loss: 1.0833 - accuracy: 0.5068 - val_loss: 1.6438 - val_accuracy: 0.3467\n",
      "Epoch 105/200\n",
      "60/60 - 1s - loss: 1.0182 - accuracy: 0.5034 - val_loss: 1.8795 - val_accuracy: 0.3467\n",
      "Epoch 106/200\n",
      "60/60 - 1s - loss: 1.0077 - accuracy: 0.5000 - val_loss: 1.8867 - val_accuracy: 0.3333\n",
      "Epoch 107/200\n",
      "60/60 - 1s - loss: 1.0146 - accuracy: 0.4932 - val_loss: 1.8777 - val_accuracy: 0.3333\n",
      "Epoch 108/200\n",
      "60/60 - 1s - loss: 1.0253 - accuracy: 0.4932 - val_loss: 2.1398 - val_accuracy: 0.3333\n",
      "Epoch 109/200\n",
      "60/60 - 1s - loss: 1.0221 - accuracy: 0.4932 - val_loss: 1.9640 - val_accuracy: 0.3333\n",
      "Epoch 110/200\n",
      "60/60 - 1s - loss: 1.0265 - accuracy: 0.4932 - val_loss: 1.7236 - val_accuracy: 0.3333\n",
      "Epoch 111/200\n",
      "60/60 - 1s - loss: 1.0183 - accuracy: 0.4899 - val_loss: 1.9596 - val_accuracy: 0.3333\n",
      "Epoch 112/200\n",
      "60/60 - 1s - loss: 1.0263 - accuracy: 0.5304 - val_loss: 1.6707 - val_accuracy: 0.3467\n",
      "Epoch 113/200\n",
      "60/60 - 1s - loss: 1.0126 - accuracy: 0.5034 - val_loss: 2.0507 - val_accuracy: 0.3333\n",
      "Epoch 114/200\n",
      "60/60 - 1s - loss: 0.9870 - accuracy: 0.5034 - val_loss: 2.0046 - val_accuracy: 0.3333\n",
      "Epoch 115/200\n",
      "60/60 - 1s - loss: 0.9755 - accuracy: 0.5068 - val_loss: 2.0476 - val_accuracy: 0.3467\n",
      "Epoch 116/200\n",
      "60/60 - 1s - loss: 0.9870 - accuracy: 0.5068 - val_loss: 2.1494 - val_accuracy: 0.3333\n",
      "Epoch 117/200\n",
      "60/60 - 1s - loss: 1.0182 - accuracy: 0.5135 - val_loss: 1.9102 - val_accuracy: 0.3333\n",
      "Epoch 118/200\n",
      "60/60 - 1s - loss: 1.0701 - accuracy: 0.4764 - val_loss: 1.6715 - val_accuracy: 0.3467\n",
      "Epoch 119/200\n",
      "60/60 - 1s - loss: 1.0210 - accuracy: 0.5135 - val_loss: 1.8795 - val_accuracy: 0.3333\n",
      "Epoch 120/200\n",
      "60/60 - 1s - loss: 1.0095 - accuracy: 0.5236 - val_loss: 1.9718 - val_accuracy: 0.3333\n",
      "Epoch 121/200\n",
      "60/60 - 1s - loss: 1.0011 - accuracy: 0.5101 - val_loss: 1.9659 - val_accuracy: 0.3333\n",
      "Epoch 122/200\n",
      "60/60 - 1s - loss: 1.0369 - accuracy: 0.4865 - val_loss: 1.6960 - val_accuracy: 0.3600\n",
      "Epoch 123/200\n",
      "60/60 - 1s - loss: 1.0389 - accuracy: 0.4932 - val_loss: 1.8066 - val_accuracy: 0.3333\n",
      "Epoch 124/200\n",
      "60/60 - 1s - loss: 0.9896 - accuracy: 0.4797 - val_loss: 2.0970 - val_accuracy: 0.3333\n",
      "Epoch 125/200\n",
      "60/60 - 1s - loss: 0.9932 - accuracy: 0.5169 - val_loss: 1.8022 - val_accuracy: 0.3333\n",
      "Epoch 126/200\n",
      "60/60 - 1s - loss: 0.9808 - accuracy: 0.5439 - val_loss: 1.9331 - val_accuracy: 0.3333\n",
      "Epoch 127/200\n",
      "60/60 - 1s - loss: 0.9642 - accuracy: 0.5270 - val_loss: 1.9992 - val_accuracy: 0.3333\n",
      "Epoch 128/200\n",
      "60/60 - 1s - loss: 0.9864 - accuracy: 0.4932 - val_loss: 1.9827 - val_accuracy: 0.3333\n",
      "Epoch 129/200\n",
      "60/60 - 1s - loss: 1.0044 - accuracy: 0.5135 - val_loss: 1.9442 - val_accuracy: 0.3333\n",
      "Epoch 130/200\n",
      "60/60 - 1s - loss: 1.0456 - accuracy: 0.5338 - val_loss: 1.7665 - val_accuracy: 0.3333\n",
      "Epoch 131/200\n",
      "60/60 - 1s - loss: 0.9961 - accuracy: 0.5203 - val_loss: 1.9101 - val_accuracy: 0.3600\n",
      "Epoch 132/200\n",
      "60/60 - 1s - loss: 0.9610 - accuracy: 0.5236 - val_loss: 2.0722 - val_accuracy: 0.3333\n",
      "Epoch 133/200\n",
      "60/60 - 1s - loss: 0.9865 - accuracy: 0.4899 - val_loss: 2.0021 - val_accuracy: 0.3333\n",
      "Epoch 134/200\n",
      "60/60 - 1s - loss: 1.0053 - accuracy: 0.5338 - val_loss: 1.6985 - val_accuracy: 0.3333\n",
      "Epoch 135/200\n",
      "60/60 - 1s - loss: 1.0153 - accuracy: 0.5169 - val_loss: 1.8690 - val_accuracy: 0.3333\n",
      "Epoch 136/200\n",
      "60/60 - 1s - loss: 0.9691 - accuracy: 0.5169 - val_loss: 2.1536 - val_accuracy: 0.3467\n",
      "Epoch 137/200\n",
      "60/60 - 1s - loss: 0.9799 - accuracy: 0.5068 - val_loss: 1.9872 - val_accuracy: 0.3333\n",
      "Epoch 138/200\n",
      "60/60 - 1s - loss: 0.9956 - accuracy: 0.5000 - val_loss: 1.9043 - val_accuracy: 0.3333\n",
      "Epoch 139/200\n",
      "60/60 - 1s - loss: 0.9849 - accuracy: 0.4966 - val_loss: 1.9554 - val_accuracy: 0.3333\n",
      "Epoch 140/200\n",
      "60/60 - 1s - loss: 0.9708 - accuracy: 0.5270 - val_loss: 2.1116 - val_accuracy: 0.3333\n",
      "Epoch 141/200\n",
      "60/60 - 1s - loss: 0.9462 - accuracy: 0.5135 - val_loss: 1.9871 - val_accuracy: 0.3333\n",
      "Epoch 142/200\n",
      "60/60 - 1s - loss: 0.9455 - accuracy: 0.5405 - val_loss: 2.0179 - val_accuracy: 0.3200\n",
      "Epoch 143/200\n",
      "60/60 - 1s - loss: 0.9443 - accuracy: 0.5439 - val_loss: 1.9460 - val_accuracy: 0.3333\n",
      "Epoch 144/200\n",
      "60/60 - 1s - loss: 0.9833 - accuracy: 0.5169 - val_loss: 2.1192 - val_accuracy: 0.3067\n",
      "Epoch 145/200\n",
      "60/60 - 1s - loss: 1.0106 - accuracy: 0.5068 - val_loss: 1.8399 - val_accuracy: 0.3333\n",
      "Epoch 146/200\n",
      "60/60 - 1s - loss: 1.0521 - accuracy: 0.5068 - val_loss: 1.6251 - val_accuracy: 0.3467\n",
      "Epoch 147/200\n",
      "60/60 - 1s - loss: 0.9942 - accuracy: 0.5203 - val_loss: 2.0489 - val_accuracy: 0.3333\n",
      "Epoch 148/200\n",
      "60/60 - 1s - loss: 0.9510 - accuracy: 0.5372 - val_loss: 2.1293 - val_accuracy: 0.3333\n",
      "Epoch 149/200\n",
      "60/60 - 1s - loss: 0.9601 - accuracy: 0.4797 - val_loss: 2.3232 - val_accuracy: 0.3467\n",
      "Epoch 150/200\n",
      "60/60 - 1s - loss: 0.9544 - accuracy: 0.4696 - val_loss: 2.2705 - val_accuracy: 0.3200\n",
      "Epoch 151/200\n",
      "60/60 - 1s - loss: 0.9750 - accuracy: 0.4966 - val_loss: 2.0394 - val_accuracy: 0.3333\n",
      "Epoch 152/200\n",
      "60/60 - 1s - loss: 0.9814 - accuracy: 0.5338 - val_loss: 1.9248 - val_accuracy: 0.3467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/200\n",
      "60/60 - 1s - loss: 0.9560 - accuracy: 0.5236 - val_loss: 1.9076 - val_accuracy: 0.3333\n",
      "Epoch 154/200\n",
      "60/60 - 1s - loss: 0.9724 - accuracy: 0.5203 - val_loss: 2.5101 - val_accuracy: 0.3333\n",
      "Epoch 155/200\n",
      "60/60 - 1s - loss: 0.9473 - accuracy: 0.5372 - val_loss: 2.1453 - val_accuracy: 0.3467\n",
      "Epoch 156/200\n",
      "60/60 - 1s - loss: 0.9594 - accuracy: 0.5338 - val_loss: 2.2025 - val_accuracy: 0.3200\n",
      "Epoch 157/200\n",
      "60/60 - 1s - loss: 0.9562 - accuracy: 0.5372 - val_loss: 2.0517 - val_accuracy: 0.3467\n",
      "Epoch 158/200\n",
      "60/60 - 1s - loss: 0.9887 - accuracy: 0.5000 - val_loss: 1.9309 - val_accuracy: 0.3600\n",
      "Epoch 159/200\n",
      "60/60 - 1s - loss: 0.9896 - accuracy: 0.4865 - val_loss: 1.9475 - val_accuracy: 0.3333\n",
      "Epoch 160/200\n",
      "60/60 - 1s - loss: 0.9681 - accuracy: 0.5405 - val_loss: 2.0844 - val_accuracy: 0.3200\n",
      "Epoch 161/200\n",
      "60/60 - 1s - loss: 0.9448 - accuracy: 0.5541 - val_loss: 2.3394 - val_accuracy: 0.3333\n",
      "Epoch 162/200\n",
      "60/60 - 1s - loss: 0.9183 - accuracy: 0.5304 - val_loss: 2.2739 - val_accuracy: 0.3200\n",
      "Epoch 163/200\n",
      "60/60 - 1s - loss: 0.9979 - accuracy: 0.5101 - val_loss: 1.7721 - val_accuracy: 0.3467\n",
      "Epoch 164/200\n",
      "60/60 - 1s - loss: 1.0324 - accuracy: 0.5068 - val_loss: 1.8403 - val_accuracy: 0.3333\n",
      "Epoch 165/200\n",
      "60/60 - 1s - loss: 0.9829 - accuracy: 0.5405 - val_loss: 1.7609 - val_accuracy: 0.3467\n",
      "Epoch 166/200\n",
      "60/60 - 1s - loss: 0.9543 - accuracy: 0.5439 - val_loss: 1.8547 - val_accuracy: 0.3333\n",
      "Epoch 167/200\n",
      "60/60 - 1s - loss: 0.9596 - accuracy: 0.4966 - val_loss: 1.8508 - val_accuracy: 0.3333\n",
      "Epoch 168/200\n",
      "60/60 - 1s - loss: 0.9392 - accuracy: 0.5068 - val_loss: 1.8992 - val_accuracy: 0.3333\n",
      "Epoch 169/200\n",
      "60/60 - 1s - loss: 0.9277 - accuracy: 0.5507 - val_loss: 2.0360 - val_accuracy: 0.3200\n",
      "Epoch 170/200\n",
      "60/60 - 1s - loss: 0.9423 - accuracy: 0.5068 - val_loss: 1.9548 - val_accuracy: 0.3467\n",
      "Epoch 171/200\n",
      "60/60 - 1s - loss: 0.9097 - accuracy: 0.5777 - val_loss: 2.1342 - val_accuracy: 0.3200\n",
      "Epoch 172/200\n",
      "60/60 - 1s - loss: 0.9578 - accuracy: 0.5304 - val_loss: 2.1623 - val_accuracy: 0.3600\n",
      "Epoch 173/200\n",
      "60/60 - 1s - loss: 1.0230 - accuracy: 0.5034 - val_loss: 2.0854 - val_accuracy: 0.3333\n",
      "Epoch 174/200\n",
      "60/60 - 1s - loss: 0.9946 - accuracy: 0.5236 - val_loss: 1.7145 - val_accuracy: 0.3333\n",
      "Epoch 175/200\n",
      "60/60 - 1s - loss: 0.9834 - accuracy: 0.5541 - val_loss: 1.9876 - val_accuracy: 0.3333\n",
      "Epoch 176/200\n",
      "60/60 - 1s - loss: 0.9502 - accuracy: 0.5270 - val_loss: 2.0697 - val_accuracy: 0.3333\n",
      "Epoch 177/200\n",
      "60/60 - 1s - loss: 0.9247 - accuracy: 0.5338 - val_loss: 2.0707 - val_accuracy: 0.3200\n",
      "Epoch 178/200\n",
      "60/60 - 1s - loss: 0.9332 - accuracy: 0.5304 - val_loss: 2.1293 - val_accuracy: 0.3200\n",
      "Epoch 179/200\n",
      "60/60 - 1s - loss: 0.9492 - accuracy: 0.5203 - val_loss: 2.4844 - val_accuracy: 0.3200\n",
      "Epoch 180/200\n",
      "60/60 - 1s - loss: 0.9990 - accuracy: 0.5068 - val_loss: 1.9174 - val_accuracy: 0.3333\n",
      "Epoch 181/200\n",
      "60/60 - 1s - loss: 1.0410 - accuracy: 0.4797 - val_loss: 1.7528 - val_accuracy: 0.3333\n",
      "Epoch 182/200\n",
      "60/60 - 1s - loss: 0.9853 - accuracy: 0.5135 - val_loss: 1.8362 - val_accuracy: 0.3467\n",
      "Epoch 183/200\n",
      "60/60 - 1s - loss: 0.9407 - accuracy: 0.5068 - val_loss: 1.9059 - val_accuracy: 0.3333\n",
      "Epoch 184/200\n",
      "60/60 - 1s - loss: 0.9199 - accuracy: 0.5101 - val_loss: 2.0903 - val_accuracy: 0.3200\n",
      "Epoch 185/200\n",
      "60/60 - 1s - loss: 0.8964 - accuracy: 0.5642 - val_loss: 2.2167 - val_accuracy: 0.3200\n",
      "Epoch 186/200\n",
      "60/60 - 1s - loss: 0.9054 - accuracy: 0.5676 - val_loss: 2.3351 - val_accuracy: 0.3200\n",
      "Epoch 187/200\n",
      "60/60 - 1s - loss: 0.9075 - accuracy: 0.5405 - val_loss: 2.3524 - val_accuracy: 0.2933\n",
      "Epoch 188/200\n",
      "60/60 - 1s - loss: 0.9053 - accuracy: 0.5338 - val_loss: 2.2033 - val_accuracy: 0.3333\n",
      "Epoch 189/200\n",
      "60/60 - 1s - loss: 0.9311 - accuracy: 0.5372 - val_loss: 2.1354 - val_accuracy: 0.3067\n",
      "Epoch 190/200\n",
      "60/60 - 1s - loss: 0.9531 - accuracy: 0.5169 - val_loss: 2.5286 - val_accuracy: 0.3333\n",
      "Epoch 191/200\n",
      "60/60 - 1s - loss: 0.9457 - accuracy: 0.5338 - val_loss: 2.3517 - val_accuracy: 0.3200\n",
      "Epoch 192/200\n",
      "60/60 - 1s - loss: 0.9511 - accuracy: 0.5270 - val_loss: 1.9228 - val_accuracy: 0.3467\n",
      "Epoch 193/200\n",
      "60/60 - 1s - loss: 0.9604 - accuracy: 0.5304 - val_loss: 2.0243 - val_accuracy: 0.3200\n",
      "Epoch 194/200\n",
      "60/60 - 1s - loss: 0.9249 - accuracy: 0.5439 - val_loss: 2.2658 - val_accuracy: 0.3467\n",
      "Epoch 195/200\n",
      "60/60 - 1s - loss: 0.9002 - accuracy: 0.5338 - val_loss: 2.3424 - val_accuracy: 0.2933\n",
      "Epoch 196/200\n",
      "60/60 - 1s - loss: 0.8731 - accuracy: 0.5405 - val_loss: 2.3491 - val_accuracy: 0.2933\n",
      "Epoch 197/200\n",
      "60/60 - 1s - loss: 0.9493 - accuracy: 0.5304 - val_loss: 2.5785 - val_accuracy: 0.3200\n",
      "Epoch 198/200\n",
      "60/60 - 1s - loss: 0.9172 - accuracy: 0.5574 - val_loss: 2.3748 - val_accuracy: 0.3333\n",
      "Epoch 199/200\n",
      "60/60 - 1s - loss: 0.9216 - accuracy: 0.5203 - val_loss: 2.3668 - val_accuracy: 0.3333\n",
      "Epoch 200/200\n",
      "60/60 - 1s - loss: 0.9016 - accuracy: 0.5574 - val_loss: 2.2684 - val_accuracy: 0.3333\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# design network\n",
    "model = Sequential()\n",
    "neurons = 32\n",
    "epochs = 200\n",
    "dropout = 0.25\n",
    "batch_size = 5\n",
    "activ_func = \"linear\"\n",
    "\n",
    "model.add(LSTM(neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2]), activation=activ_func))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(LSTM(neurons, return_sequences=True, activation=activ_func))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(LSTM(neurons, return_sequences=False, activation=activ_func))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, verbose=2, shuffle=False,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict values for test data\n",
    "if(n_lag > 0):\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_lag, n_features))\n",
    "else:\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, n_features-1))\n",
    "\n",
    "pred = model.predict(test_X)\n",
    "\n",
    "if(n_lag > 0):\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_lag* n_features,))\n",
    "else:\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_features-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change back from categorical\n",
    "pred = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = sklearn.metrics.classification_report(test_y, pred, zero_division=0,output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34545454545454546"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = np.zeros(10)\n",
    "for i in range(0,10):\n",
    "    f1[i] = report[str(i)]['f1-score'] if str(i) in report else 0\n",
    "\n",
    "report['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         6\n",
      "           4       0.00      0.00      0.00        24\n",
      "           5       0.35      0.95      0.51        20\n",
      "           6       0.00      0.00      0.00         4\n",
      "           8       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.35        55\n",
      "   macro avg       0.07      0.19      0.10        55\n",
      "weighted avg       0.13      0.35      0.19        55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(test_y, pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
