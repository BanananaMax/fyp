{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to test the paramters of the LSTM trend model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "import sklearn\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Bidirectional\n",
    "# from keras.layers import CuDNNLSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "import keras.utils\n",
    "from keras.layers import LSTM\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import tensorflow as tf\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import seed\n",
    "import os\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    \n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "        \n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate rsi\n",
    "def rsi(ohlc, period: int = 14) -> pd.Series:\n",
    "    \"\"\"See source https://github.com/peerchemist/finta\n",
    "    and fix https://www.tradingview.com/wiki/Talk:Relative_Strength_Index_(RSI)\n",
    "    Relative Strength Index (RSI) is a momentum oscillator that measures the speed and change of price movements.\n",
    "    RSI oscillates between zero and 100. Traditionally, and according to Wilder, RSI is considered overbought when above 70 and oversold when below 30.\n",
    "    Signals can also be generated by looking for divergences, failure swings and centerline crossovers.\n",
    "    RSI can also be used to identify the general trend.\"\"\"\n",
    "\n",
    "    delta = ohlc[\"Close\"].diff()\n",
    "\n",
    "    up, down = delta.copy(), delta.copy()\n",
    "    up[up < 0] = 0\n",
    "    down[down > 0] = 0\n",
    "\n",
    "    _gain = up.ewm(com=(period - 1), min_periods=period).mean()\n",
    "    _loss = down.abs().ewm(com=(period - 1), min_periods=period).mean()\n",
    "\n",
    "    RS = _gain / _loss\n",
    "    return 100 - (100 / (1 + RS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols_to_rem_label(n_lag, n_features, label_col):\n",
    "    to_rem = []\n",
    "    for i in range(1, n_features+1):\n",
    "        for j in range(0, n_lag+1):\n",
    "            if(j!=0 or i != label_col):\n",
    "                if(j == 0):\n",
    "                    to_rem.append(\"var\"+str(i)+\"(t)\")\n",
    "                else:\n",
    "                    to_rem.append(\"var\"+str(i)+\"(t-\"+str(j)+\")\")\n",
    "    return to_rem\n",
    "    \n",
    "def get_cols_to_rem_feat(n_lag, n_features, label_col):\n",
    "    #uncomment the below line to only remove the last price\n",
    "    to_rem = [\"var1(t)\", \"var2(t)\"]\n",
    "    return to_rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_test(epochs, neurons, batch_size, layers, train_X, test_X, train_y, test_y, lag_features, features, df, train_size):\n",
    "    global n_lag\n",
    "    global n_features\n",
    "\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    dropout = 0.25\n",
    "    activ_func = \"linear\"\n",
    "    \n",
    "    return_seq = layers > 1\n",
    "\n",
    "    model.add(LSTM(neurons, return_sequences=return_seq, input_shape=(train_X.shape[1], train_X.shape[2]), activation=activ_func))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    for i in range(1, layers):\n",
    "        ret_seq = i != (layers-1)\n",
    "        model.add(LSTM(neurons, return_sequences=ret_seq, activation=activ_func))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "#     model.summary()\n",
    "\n",
    "\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # fit network\n",
    "    history = model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False,validation_split=0.2)\n",
    "    \n",
    "    if(lag_features > 0):\n",
    "        test_X = test_X.reshape((test_X.shape[0], lag_features, features))\n",
    "    else:\n",
    "        test_X = test_X.reshape((test_X.shape[0], 1, features-1))\n",
    "\n",
    "    pred = model.predict(test_X)\n",
    "\n",
    "    if(lag_features > 0):\n",
    "        test_X = test_X.reshape((test_X.shape[0], lag_features* features,))\n",
    "    else:\n",
    "        test_X = test_X.reshape((test_X.shape[0], features-1,))\n",
    "        \n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    \n",
    "    times = pd.Series(df.index)\n",
    "    times = times[lag_features:]\n",
    "    times = times[train_size:]\n",
    "    \n",
    "    prices = pd.DataFrame()\n",
    "    prices[\"Actual\"] = test_y\n",
    "    prices[\"Predicted\"] = pred\n",
    "    prices[\"Correct\"] = (prices[\"Actual\"] - prices[\"Predicted\"]) == 0\n",
    "    incorrect = prices.loc[prices['Correct'] == False]\n",
    "    incorrect_len = len(incorrect)\n",
    "    prices_len = len(prices)\n",
    "    accuracy = ((prices_len-incorrect_len)/prices_len)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lag_sets(lag_features, train_ratio, lag_granularity, lag, dataset_grouped_by, cleaned):\n",
    "    # type of analyser - TextBlob or vader\n",
    "    analyser = \"vader\"\n",
    "    # analyser = \"TextBlob\"\n",
    "    \n",
    "    #read dataset\n",
    "    folder = \"./../datasets/tweets_prices_volumes_sentiment/\"+analyser+\"/\"+dataset_grouped_by+\"_datasets\"\n",
    "    \n",
    "    if cleaned:\n",
    "        folder = folder + '/cleaned'\n",
    "        \n",
    "    print(folder)        \n",
    "    filename = folder+\"/final_data_lag_\"+lag_granularity+\"_\"+str(lag)+\".csv\" if (lag > 0) else folder+\"/final_data_no_lag.csv\"\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    #group by datetime\n",
    "    df = df.groupby('DateTime').agg(lambda x: x.mean())\n",
    "    \n",
    "    df[\"Change\"] = (df[\"Close\"] > df[\"Close\"].shift(1)).astype(int)\n",
    "\n",
    "    add_RSI = False\n",
    "    add_longMAvg = False\n",
    "    add_shortMAvg = False\n",
    "\n",
    "    if(add_RSI):\n",
    "        #calcualte RSI\n",
    "        RSI = 14\n",
    "        df['RSI'] = rsi(df, RSI)\n",
    "        df = df.iloc[RSI:]\n",
    "\n",
    "    #calcualte moving averages\n",
    "\n",
    "    if(add_shortMAvg):\n",
    "        short_window = 9\n",
    "        df['short_mavg'] = df.rolling(window=short_window)[\"Close\"].mean()\n",
    "\n",
    "    if(add_longMAvg):\n",
    "        long_window = 21\n",
    "        df[\"long_mavg\"] = df.rolling(window=long_window)[\"Close\"].mean()\n",
    "\n",
    "    if(add_longMAvg):\n",
    "        df = df.iloc[long_window:]\n",
    "    elif(add_RSI):\n",
    "        df = df.iloc[RSI:]\n",
    "    elif(add_shortMAvg):\n",
    "        df = df.iloc[short_window:]\n",
    "        \n",
    "        \n",
    "    #keep only wanted columns\n",
    "    features = ['Change', 'subjectivity', 'polarity','Tweet_vol','Volume_(BTC)'] if analyser == \"Textblob\" else ['Change', 'Close', 'pos_pol', 'neg_pol', 'Tweet_vol']\n",
    "\n",
    "    # features = ['Change', 'subjectivity', 'polarity','Tweet_vol','Volume_(BTC)'] if analyser == \"Textblob\" else ['Change', 'Close', 'compound', 'pos_pol', 'neg_pol', 'neu_pol', 'Tweet_vol','Volume_(BTC)']\n",
    "\n",
    "    if(add_RSI):\n",
    "        features.append(\"RSI\")\n",
    "\n",
    "    if(add_longMAvg):\n",
    "        features.append(\"long_mavg\")\n",
    "\n",
    "    if(add_shortMAvg):\n",
    "        features.append(\"short_mavg\")\n",
    "\n",
    "    df = df[features]\n",
    "\n",
    "    #creating copy so that data is not loaded once again\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    #number of previous records to consider for every example\n",
    "    n_lag = lag_features\n",
    "    #number of features\n",
    "    n_features = len(features)\n",
    "    #calcualte total_features\n",
    "    total_features = n_lag*n_features\n",
    "\n",
    "    if(total_features == 0):\n",
    "        total_features = n_features\n",
    "        \n",
    "    #divide df into train and test\n",
    "    data_len = len(df_copy)\n",
    "    train_size = int(data_len*train_ratio)    \n",
    "\n",
    "    train = df_copy.iloc[:train_size]\n",
    "    test = df_copy.iloc[train_size:]\n",
    "    \n",
    "    train_y = train[\"Change\"][n_lag:].values\n",
    "    test_y = test[\"Change\"][n_lag:].values\n",
    "\n",
    "    train_y = train_y.reshape(len(train_y), 1)\n",
    "    test_y = test_y.reshape(len(test_y), 1)\n",
    "    \n",
    "    xscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train = xscaler.fit_transform(train)\n",
    "    test = xscaler.transform(test)\n",
    "\n",
    "    train_reframed = series_to_supervised(train, n_lag, 1)\n",
    "    train_reframed =  train_reframed.reset_index()\n",
    "    train_reframed = train_reframed.drop(['index'], axis=1)\n",
    "\n",
    "    test_reframed = series_to_supervised(test, n_lag, 1)\n",
    "    test_reframed =  test_reframed.reset_index()\n",
    "    test_reframed = test_reframed.drop(['index'], axis=1)\n",
    "\n",
    "    train_reframed_labels = train_y\n",
    "    test_reframed_labels = test_y\n",
    "    \n",
    "    train_labels = train_reframed.drop(get_cols_to_rem_feat(n_lag, n_features, 1), axis=1)\n",
    "    test_labels = test_reframed.drop(get_cols_to_rem_feat(n_lag, n_features, 1), axis=1)\n",
    "\n",
    "    train = train_reframed.values\n",
    "    test = test_reframed.values\n",
    "    \n",
    "    #remove the last set of values(data of time to be predicted)\n",
    "    train = train[:, :total_features]\n",
    "    test = test[:, :total_features]\n",
    "\n",
    "    #keep only prices array\n",
    "    train_X, train_y = train[:, :total_features], train_y[:, -1]\n",
    "    test_X, test_y = test[:, :total_features], test_y[:, -1]\n",
    "\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    if(n_lag > 0):\n",
    "        train_X = train_X.reshape((train_X.shape[0], n_lag, n_features))\n",
    "        test_X = test_X.reshape((test_X.shape[0], n_lag, n_features))\n",
    "    else:\n",
    "        train_X = train_X.reshape((train_X.shape[0], 1, n_features-1))\n",
    "        test_X = test_X.reshape((test_X.shape[0], 1, n_features-1))\n",
    "\n",
    "    train_y = keras.utils.to_categorical(train_y, 2)\n",
    "    \n",
    "    return train_X, test_X, train_y, test_y, len(features), df, train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(lag_granularity, lag, dataset_grouped_by, cleaned, epochs, layers, neurons, batch_size, lag_features):\n",
    "    #get filename\n",
    "    filename = 'results/lstm_groupedby_'+dataset_grouped_by+\"_lag_\"+lag_granularity+\"_\"+str(lag)\n",
    "\n",
    "    if cleaned:\n",
    "        filename = filename + '_cleaned'\n",
    "\n",
    "    full_filename = filename+\".csv\"\n",
    "    \n",
    "    train_ratio = 0.85\n",
    "    \n",
    "    train_X, test_X, train_y, test_y, features, df, train_size = load_lag_sets(lag_features, train_ratio, lag_granularity, lag, dataset_grouped_by, cleaned)\n",
    "\n",
    "    print(\"Testing model: lag:\", lag_features, \", epochs:\", epochs, \", neurons:\", neurons, \", layers:\", layers, \", batch_size:\", batch_size)\n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range (0,10):\n",
    "        acc = create_model_test(epochs, neurons, batch_size, lag_features, train_X, test_X, train_y, test_y, lag_features, features, df, train_size)\n",
    "        accuracies.append(acc)\n",
    "        print(\"Run\", (i+1), \":\", acc)\n",
    "\n",
    "    return np.array(accuracies).mean()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lag granularity - days or hours\n",
    "lag_granularity = \"days\"\n",
    "#lag value\n",
    "lag = 3\n",
    "#dataset grouped type - day or hour\n",
    "dataset_grouped_by = \"day\"\n",
    "#cleaned\n",
    "cleaned = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "feature_lag = 14\n",
    "neurons = 32\n",
    "layers = 3\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../datasets/tweets_prices_volumes_sentiment/vader/day_datasets/cleaned\n",
      "Testing model: lag: 14 , epochs: 1000 , neurons: 32 , layers: 3 , batch_size: 5\n",
      "Epoch 1/1000\n",
      "60/60 [==============================] - 7s 122ms/step - loss: 0.6848 - accuracy: 0.5304 - val_loss: 0.7776 - val_accuracy: 0.5067\n",
      "Epoch 2/1000\n",
      "60/60 [==============================] - 4s 70ms/step - loss: 0.7032 - accuracy: 0.5439 - val_loss: 0.6965 - val_accuracy: 0.5067\n",
      "Epoch 3/1000\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.6882 - accuracy: 0.5574 - val_loss: 0.7007 - val_accuracy: 0.5067\n",
      "Epoch 4/1000\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.6905 - accuracy: 0.5574 - val_loss: 0.6996 - val_accuracy: 0.5067\n",
      "Epoch 5/1000\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.6906 - accuracy: 0.5574 - val_loss: 0.7002 - val_accuracy: 0.5067\n",
      "Epoch 6/1000\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.6892 - accuracy: 0.5574 - val_loss: 0.7007 - val_accuracy: 0.5067\n",
      "Epoch 7/1000\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.6885 - accuracy: 0.5574 - val_loss: 0.7006 - val_accuracy: 0.5067\n",
      "Epoch 8/1000\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.6898 - accuracy: 0.5574 - val_loss: 0.7001 - val_accuracy: 0.5067\n",
      "Epoch 9/1000\n",
      "60/60 [==============================] - 4s 70ms/step - loss: 0.6883 - accuracy: 0.5574 - val_loss: 0.7010 - val_accuracy: 0.5067\n",
      "Epoch 10/1000\n",
      "60/60 [==============================] - 4s 70ms/step - loss: 0.6892 - accuracy: 0.5574 - val_loss: 0.7000 - val_accuracy: 0.5067\n",
      "Epoch 11/1000\n",
      " 6/60 [==>...........................] - ETA: 3s - loss: 0.7328 - accuracy: 0.4000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5962157e19f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlag_granularity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_grouped_by\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcleaned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneurons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_lag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-0f5fe0accb66>\u001b[0m in \u001b[0;36maverage\u001b[1;34m(lag_granularity, lag, dataset_grouped_by, cleaned, epochs, layers, neurons, batch_size, lag_features)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneurons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlag_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlag_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Run\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\":\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-6477e783b559>\u001b[0m in \u001b[0;36mcreate_model_test\u001b[1;34m(epochs, neurons, batch_size, layers, train_X, test_X, train_y, test_y, lag_features, features, df, train_size)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# fit network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlag_features\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\fyp\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\fyp\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\fyp\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\fyp\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\fyp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\fyp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\fyp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\fyp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\fyp\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "average(lag_granularity, lag, dataset_grouped_by, cleaned, epochs, layers, neurons, batch_size, feature_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
